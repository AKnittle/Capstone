---
title: "Capstone: Logistical Regression 2"
author: "Andrew Knittle"
date: "`r paste('Generated on', Sys.Date())`" 
output: 
  html_document: 
    # try readable or cosmo
    # default (default), try kable, tibble, or paged as an option
    # Options are none (default), show, and hide
    # default is null, try my_style_file.css
    toc: TRUE
    toc_depth: 3
    toc_float: FALSE
    highlight: haddock
    theme: flatly
    df_print: paged
    code_folding: none
    self_contained: TRUE
---

```{r setup, include=FALSE}
# --------------------------------------
# Clean and Prep Environment
rm(list=ls())
set.seed(123)

# --------------------------------------
# Load Libraries
library(pacman)
p_load(ggplot2)
p_load(dplyr)
p_load(psych)
p_load(ggpubr)
p_load(kableExtra)
p_load(corrplot)
p_load(tidyverse)
p_load(caret)
p_load(MASS)
p_load(ISLR)
p_load(glm.predict)
p_load(glmnet)
p_load(modelr)
p_load(boot)
p_load(pROC)

# --------------------------------------
# Read Data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

dustData = as.data.frame(read.csv('Data/dustData_Reduced.csv',header=T,sep=',')) # OG dataset
dustData <- dustData[-1]
dustData$CASESTAT <- as.factor(dustData$CASESTAT)
dustData <- na.omit(dustData[,c(1:27, 30)])

logDustData = as.data.frame(read.csv('Data/logDustData_Reduced.csv',header=T,sep=','))
logDustData <- logDustData[-1]
logDustData <- logDustData[-1]
logDustData$CASESTAT <- as.factor(logDustData$CASESTAT)
logDustData <- na.omit(logDustData[,c(1:27, 30)])

train_DustData <- as.data.frame(read.csv('Data/Training Set_0.9_May 25 2021.csv',header=T,sep=','))[-1]
#train_DustData$CASESTAT <- as.numeric(train_DustData$CASESTAT)
train_DustData$CASESTAT <- as.factor(train_DustData$CASESTAT)
train_DustData <- na.omit(train_DustData[,c(1:27, 30)])

# --------------------------------------
# Read helper script
source("Exploratory_Support_Scripts.R")
source("CrossValidation_Library.R")
source("Script_Library.R")


```

This is a second logistic regression report focusing on **Risk Prediction** instead of diagnosing on whether someone will develop cancer or not. We start the same way we did before of building a model (In this case Logistic using Stepwise-AIC)


# Method

Even though Logistic Regression is old hat it's not bad to review of how it works. A [great write up on Logistic Regression can be found here](https://christophm.github.io/interpretable-ml-book/logistic.html), but I will do my best to summarize without being too dense.

To start off let's answer why we should use logistic regression, instead of say Linear Regression (for those who already know why, or those who think they're "too clever" can skip this paragraph). Well Linear Regression tries to answer the question of "given some value of a set of feature(s)/independent variable(s)/predictor(s) what amount will dependent variable/response likely be". You can probably see already where this is going, but it doesn't give you a binary, or "yes or no", result which is what we are looking for in this project. We want to determine if you have cancer, not how much cancer you have.

Logistic Regression models the "*probabilities* for classification problems with two possible outcomes." In order to do this regression model uses it's name sake, the logistic function:

$$
f(x)=\frac{L}{1+e^{-k(x-x_0)}}
$$

* $f(x)$ = output of the function

* $L$ = the curve's maximum value

* $k$	=	logistic growth rate or steepness of the curve

* $x_0$ = the x value of the sigmoid midpoint

* $x$ = real number

To make it into a logistic regression model we take the foundation and tweak it a little to use predictor variables, and since this is for classification purposes we want probabilities between 0 and 1 (so $L=1$).

$$
P(y^{i}=1)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1^i+...+\beta_px_p^p)}}
$$

Now that our output is in probability, between 0 and 1, the weights are no longer in linear form. The weights/coefficients are now in terms of log odds, "probability of event divided by probability of no event". This does make interpreting your model obviously pretty difficult since it's not a simple weighted value for coefficients.

# Performance

```{r Stepwise Model, echo=FALSE, fig.cap= "Stepwise Model", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}


# Using the full dataset with log transformed data
stepwiseModel2 <- glm(CASESTAT ~ ., data = logDustData, family = "binomial") %>% stepAIC(trace = FALSE)
stepwiseModel2$formula
stepSummary <- summary(stepwiseModel2)
quickDFPrint(as.data.frame(stepSummary$coefficients))
```


```{r Cross Validation, echo=FALSE, fig.cap= "Cross Validation", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}

cvResult <- crossValidator.byKPercent(df = logDustData, response = "CASESTAT", 
                                      formula = stepwiseModel2$formula, tol = 0.5, family="binomial")



validCross <- cvResult[[4]]
errorsDF <- cvResult[[1]]

# Print results of Logistic
errorSummary <- crossError(errorsDF)
quickDFPrint(errorSummary[[1]])
errorSummary[[2]]

confuseDF <- rawConfusionBuilder(validCross$Response, validCross$holdout.tolPredict)
quickDFPrint(confuseDF[[1]])
quickDFPrint(confuseDF[[2]])

# Plot Results
# logisticPlot <- ggplot(validCross) +
#   aes(x = holdout.rawPredict, y = Response,colour = holdout.tolPredict) +
#   geom_point(shape = "circle", size = 1.5) +
#   scale_color_hue(direction = -1) +
#   labs( x = "Predicted Probability",y = "Response (Cancer)",
#     title = "Fitted Vs Observed Values",color = "Predicted Classification") +
#   theme_bw() + xlim(0L, 1L) + ylim(0L, 1L)
# logisticPlot


```


# Conclusion


# Risk Prediction

```{r Risk Prediction, echo=FALSE, fig.cap= "Risk Prediction", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}

# Plot Deciles
plotDeciles(validCross)

# Plot Deciles by Quantiles
decileQuant <- decilesByQuantiles(validCross$holdout.rawPredict, validCross$Response)
decileQuantDF <- decileQuant[[1]]
decileQuant[[2]]
decileQuant[[3]]
quickDFPrint(decileQuantDF)

# TODO: Take average of p_prime within each bin for x-axis

```




















