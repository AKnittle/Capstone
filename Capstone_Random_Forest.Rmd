---
title: "Capstone: Random Forest"
author: "Andrew Knittle"
date: "`r paste('Generated on', Sys.Date())`" 
output: 
  html_document: 
    # try readable or cosmo
    # default (default), try kable, tibble, or paged as an option
    # Options are none (default), show, and hide
    # default is null, try my_style_file.css
    toc: TRUE
    toc_depth: 3
    toc_float: FALSE
    highlight: haddock
    theme: flatly
    df_print: paged
    code_folding: none
    self_contained: TRUE
---

```{r setup, include=FALSE}
# --------------------------------------
# Clean and Prep Environment
rm(list=ls())
set.seed(123)

# --------------------------------------
# Load Libraries
library(pacman)
p_load(ggplot2)
p_load(ISLR)
p_load(dplyr)
p_load(psych)
p_load(ggpubr)
p_load(kableExtra)
p_load(tidyverse)
p_load(caret)
p_load(MASS)
p_load(glm.predict)
p_load(glmnet)
p_load(leaps)
p_load(partykit)
p_load(randomForest)
# TODO: The package ROCR can ploxt ROC curves and calculate AUC... might want to revisit a few things
p_load(ROCR)

# --------------------------------------
# Read Data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
logDustData = as.data.frame(read.csv('Data/logDustData_Reduced.csv',header=T,sep=','))
logDustData <- logDustData[-1]
logDustData$CASESTAT <- as.factor(logDustData$CASESTAT)
logDustData <- na.omit(logDustData)

train_DustData <- as.data.frame(read.csv('Data/Training Set_0.9_May 25 2021.csv',header=T,sep=','))[-1]
#train_DustData$CASESTAT <- as.numeric(train_DustData$CASESTAT)
train_DustData$CASESTAT <- as.factor(train_DustData$CASESTAT)
train_DustData <- na.omit(train_DustData[,c(1:27, 30)])
# --------------------------------------
# Read helper script
source("Exploratory_Support_Scripts.R")
source("Script_Library.R")
source("Model_Building_Library.R")



```


# What is Random Forest and how does it work?

## Background: Decision Trees

![Image name](https://miro.medium.com/max/1000/1*LMoJmXCsQlciGTEyoSN39g.jpeg)

Before reading this section it would be worth while to read on the report on *Conditional Decision Trees* I wrote because this report builds on that. You can also read this [blog post that explains both Conditional Decision Trees and Random Forest, as I quote from it quite a bit myself](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) In short Decision Trees separate different features of data at nodes for classification (if "red" take left path, if "blue" take right path). A Tree usually consists of several branches where each leads to "another tree"; these are recursive structures that contain same pattern repeated until some edge case in the form of a leaf node.

## Random Forests

Random Forests take Decision Trees and make well a "forest" of them. Each individual tree in the forest returns a classification result (A or B) and the result with the most votes is what the forest predicts classification as. For example if a forest of 10 trees is used to classify something as *Red* or *Blue*, and 7 trees returned *Blue* then the forest would return *Blue*. The results get democratized!   

![Image name](https://miro.medium.com/max/1000/1*VHDtVaDPNepRglIAv72BFg.jpeg)

Random Forests work when a ["large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models."](https://towardsdatascience.com/understanding-random-forest-58381e0602d2) The most important part of a forest working well is that each tree has low correlation between other trees. You can think of each tree as a juror, you want a diverse set of people as jurors because together as collection should cancel out any other juror's bias; the last thing you want in a fair trial is a jury that is rigged!

For a good performing tree it needs to follow two criteria:

* 1: [There needs to be some actual signal in our features so that models built using those features do better than random guessing.](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)

* 2: [The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)

### Diverse Trees

In order to make a forest diverse, in other words making sure each tree has low correlation with each other, we can use two well used options: **Bagging (Bootstrap Aggregation)** or **Feature Randomness**.


#### Bagging

<Each tree samples with replacement>

#### Feature Randomness

<Some get some features and others don't>

![Image name](https://miro.medium.com/max/1000/1*EemYMyOADnT0lJWSXmTDdg.jpeg)




# Building the Model

[I recommend reading this blog post for the intricate details and how to make your own Random Forest in R](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/)

```{r Model Building 1, echo=FALSE, fig.cap= "Model Building 1", out.height="100%", out.width="100%", warning=FALSE,  comment=FALSE, warning=FALSE}

# Build a random forest with all variables
# ntree is the number of trees in the forest
# mtry is number of random features selected per tree
# importance enables the algorithm to calculate variable importance (Not set here)
rfCancer_t100f5 <- randomForest(CASESTAT ~ ., train_DustData, ntree=100, mtry=5)
rfCancer_t100f10 <- randomForest(CASESTAT ~ ., train_DustData, ntree=100, mtry=10)
rfCancer_t100f15 <- randomForest(CASESTAT ~ ., train_DustData, ntree=100, mtry=15)

# ------------------------------------------------
# OOB = Out Of Bag Error
rfCancer_t100f5
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t100f5$predicted)))

# ------------------------------------------------

rfCancer_t100f10
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t100f10$predicted)))

# ------------------------------------------------

rfCancer_t100f15
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t100f15$predicted)))


# ROC and AUC graphs

pred_rfCancer_t100f5 <- predict(rfCancer_t100f5,logDustData,type="prob")
# pred_rfCancer_t100f5 <- na.omit(pred_rfCancer_t100f5)
classes <- levels(logDustData$CASESTAT)
true_values <- ifelse(logDustData$CASESTAT==classes,1,0)

predResultsDF <- cbind.data.frame(pred_rfCancer_t100f5, true_values)
predResultsDF <- na.omit(predResultsDF)

pred <- prediction(pred_rfCancer_t100f5[,1],true_values)
perf <- performance(pred, "tpr", "fpr")
plot(perf,main="ROC Curve") 
auc.perf <- performance(pred, measure = "auc")
print(auc.perf@y.values)
# rocBuilder(logDustData$CASESTAT, pred_rfCancer_t100f5, toleranceVec = seq(0,1,0.001))

```


```{r Model Building 2, echo=FALSE, fig.cap= "Model Building 2", out.height="100%", out.width="100%", warning=FALSE,  comment=FALSE, warning=FALSE}

rfCancer_t200f5 <- randomForest(CASESTAT ~ ., train_DustData, ntree=200, mtry=5)
rfCancer_t200f10 <- randomForest(CASESTAT ~ ., train_DustData, ntree=200, mtry=10)
rfCancer_t200f15 <- randomForest(CASESTAT ~ ., train_DustData, ntree=200, mtry=15)

# ------------------------------------------------

rfCancer_t200f5
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t200f5$predicted)))

# ------------------------------------------------

rfCancer_t200f10
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t200f10$predicted)))

# ------------------------------------------------

rfCancer_t200f15
rawConfusionBuilder(train_DustData$CASESTAT, as.numeric(as.character(rfCancer_t200f15$predicted)))

```







