---
title: "Capstone: Lasso"
author: "Andrew Knittle"
date: "`r paste('Generated on', Sys.Date())`" 
output: 
  html_document: 
    # try readable or cosmo
    # default (default), try kable, tibble, or paged as an option
    # Options are none (default), show, and hide
    # default is null, try my_style_file.css
    toc: TRUE
    toc_depth: 3
    toc_float: FALSE
    highlight: haddock
    theme: flatly
    df_print: paged
    code_folding: none
    self_contained: TRUE
---

```{r setup, include=FALSE}
# --------------------------------------
# Clean and Prep Environment
rm(list=ls())
set.seed(123)

# --------------------------------------
# Load Libraries
library(pacman)
p_load(ggplot2)
p_load(ISLR)
p_load(dplyr)
p_load(psych)
p_load(ggpubr)
p_load(kableExtra)
p_load(corrplot)
p_load(tidyverse)
p_load(tidyr)
p_load(caret)
p_load(MASS)
p_load(glm.predict)
p_load(glmnet)
p_load(leaps)

# Formatting----------------------------
p_load(kable)
p_load(kableExtra)

# --------------------------------------
# Read Data
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

logDustData = as.data.frame(read.csv('Data/logDustData_Reduced.csv',header=T,sep=','))
logDustData <- logDustData[-1]
logDustData <- logDustData[-1]
logDustData$CASESTAT <- as.factor(logDustData$CASESTAT)
logDustData <- na.omit(logDustData[,c(1:27, 30)])

# --------------------------------------
# Read helper script
source("Exploratory_Support_Scripts.R")
source("Script_Library.R")
source("CrossValidation_Library.R")
source("Model_Building_Library.R")


```

# Method

[Lasso (aka Least Absolute Shrinkage and Selection Operator) is a type of regression that utilizes "shrinkage" to weed out variables that are not optimal for prediction.](https://www.statisticshowto.com/lasso-regression/) This is a great option to consider when the number of features is fairly large as it can reduce feature space significantly.

Both Ridge and Lasso regression rely on Regularization. [Regularization](https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/), is part of what makes Lasso so special as it helps counteract overfitting data by adding a penalty term, $\lambda$. For Lasso it can be written formally as:
$$
\sum_{i=1}^{n}(y_i - \sum_{j}x_{ij}\beta_j)^2 + \lambda\sum_{j=1}^{p}|\beta_j|
$$
In more layman terms it is "Residual Sum of Squares + $\lambda$ * (Sum of the absolute value of the magnitude of coefficients)"

## Ridge Regression

Before diving into Lasso we should first talk about Ridge Regression since they both function very similarly to each other. So concepts used in one can be built on for the other.

Ridge Regression is incredibly similar in terms of scope and function. The most notable difference is that while both Lasso and Ridge Regression both take in every feature used in prediction Lasso will outright eliminate variables by setting their coefficients to zero, while lasso will have their coefficients reduced to near zero. This will be explored further below.

[Ridge works](https://youtu.be/Q81RR3yKn30?t=287) by trying to find the least sum of squared residuals and then adds $\lambda*slope^2$. The "$slope^2$" adds a penalty to the traditional Least Squares and $\lambda$ determines how severe that penalty will be. This leads to the trade off by sacrificing *more bias*, but *reducing variance*. Increase lambda enough and the slope can be asymptotically zero (*this will be important to remember later for Lasso*). This allows Ridge Regression lines to be less sensitive than a Least Squares Line. The bigger the lambda ($\lambda$) the response becomes less sensitive to weight. In order to find the best $\lambda$ we utilize *Cross Validation* to find which $\lambda$ gives the least amount of variance.

[This concept can be applied to Logistic Regression; a binary response which is what we are interested in](https://youtu.be/Q81RR3yKn30?t=807). The difference between a logistic model and a linear model, is that the *Sum of the Likelihoods* are optimized instead of the Squared Residuals. This takes the form of $\text{the sum of the likelihoods} + \lambda*slope^2$. 

These concepts scale out by taking the sum of squaring the all coefficients for all the variables and then multiplying that by $\lambda$ to give us our penalty. 


## LASSO Regression

As mentioned before [Lasso is incredibly similar to Ridge with one crucial difference](https://youtu.be/NGf0voTMlcs?t=172); the penalty is now $\text{the sum of the squared residuals} + \lambda*|slope|$. Finding the best $\lambda$ is the same via Cross Validation, and the trade off that comes with the penalty is the same. Both can be scaled the same way, and both can be used to predict binary responses.

What applying absolute values to the coefficients for penalties allows is having the slope be zero, *instead of asymptotically like Ridge.* Therefore this will [eliminate terms/features that are considered "useless".](https://youtu.be/NGf0voTMlcs?t=411) This makes Lasso **slightly better** than Ridge at *reducing the variance in the model that contain a lot impractical variables*. Conversely Ridge does **slightly better** than Lasso *when most variables are considered useful.* For this project Lasso was chosen since there are a lot of variables in this data set and reducing the number of them would make interpretation much easier.

```{r Model Building, echo=FALSE, fig.cap= "Model Building", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}

# Initialize Lasso Results from method
lassoResult <- NA

# Due to randomness beyond my control, and even having the seed set throughout
# I've saved the result of building a Lasso Model with results that make sense.
# If however the file for it does not exist build it again, but no you may need
# repeat the process several times until you get satisfying results
if(!file.exists("lassoResult.rds")){
  lambda <- 10^seq(10,-2,length=30)
  lassoResult <- buildLasso(logDustData, "CASESTAT", lambda)

}else{
  lassoResult <- readRDS(file = "lassoResult.rds")
}

# Get the coefficients for Lasso and print
coefLasso <- as.matrix(coef(lassoResult$lasso.mod, s=lassoResult$bestLambda))
colnames(coefLasso) <- c("Coefficient")
coefLasso <- subset(coefLasso, coefLasso[,1] != 0)

quickDFPrint(coefLasso)

# Plot cross validation for lambda
plot(lassoResult$cv.out)

plot(lassoResult$lasso.mod, xvar = "lambda")


```

# Perfomnace

```{r Cross Validation, echo=FALSE, fig.cap= "Cross Validation", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}


lassoFit <- lassoResult$lasso.mod
bestLambda <- lassoResult$bestLambda
lasso.formula <- as.formula("CASESTAT ~ D24_1 + DICAMBA_1 + BENZ_A_PYRENE_1 + PCB180_1 + CARBARYL_1 + A_CHLORDANE_1 + CHLORPYRIFOS_1 + DDE_1 + DDT_1 + DIAZINON_1 + METHOXYCHLOR_1 + PROPOXUR_1 + PENTACHLOROPHENOL_1")

lassoCross <- crossValidator.byKPercent(df = logDustData, response = "CASESTAT", 
                                        formula = lasso.formula, family="lasso",bestLambda=bestLambda, tol=0.5)
# Print results of Lasso
errorsDF <- lassoCross[[1]]
errorSummary <- crossError(errorsDF)
quickDFPrint(errorSummary[[1]])
errorSummary[[2]]

validCross <- lassoCross[[4]]
confuseDF <- rawConfusionBuilder(validCross$Response, validCross$holdout.tolPredict)
quickDFPrint(confuseDF[[1]])
quickDFPrint(confuseDF[[2]])
```

# Conclusion

# Risk Prediction

```{r Risk Prediction, echo=FALSE, fig.cap= "Risk Prediction", out.height="100%", out.width="100%", warning=FALSE, comment=FALSE, warning=FALSE}

# Plot Deciles
plotDeciles(validCross)

# Plot Deciles by Quantiles
decileQuant <- decilesByQuantiles(validCross$holdout.rawPredict, validCross$Response)
decileQuantDF <- decileQuant[[1]]
decileQuant[[2]]
decileQuant[[3]]
decileQuantDF

```










